{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5374a-bf87-4e47-94b9-aa114a130e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CuPy (GPU acceleration)\n",
      "\n",
      "parameters: dendrite_mask: 6944.0, dendrite_b: 434, soma_W: 434.0, soma_b: 14\n",
      "number of model_1 params: 7976\n",
      "\n",
      "parameters: dendrite_mask: 6944.0, dendrite_b: 434, soma_W: 434.0, soma_b: 14\n",
      "number of model_2 params: 7976\n",
      "number of model_3 params: 8070\n",
      "Loading FASHION-MNIST dataset...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import cupy as cp\n",
    "\n",
    "    # Test if CuPy can actually access CUDA and random number generator\n",
    "    cp.cuda.Device(0).compute_capability\n",
    "    cp.random.seed(1)  # Test if random number generator works\n",
    "    print(\"Using CuPy (GPU acceleration)\")\n",
    "except (ImportError, Exception) as e:\n",
    "    import numpy as cp\n",
    "\n",
    "    print(f\"CuPy not available or CUDA error ({type(e).__name__}), using NumPy (CPU)\")\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.softmax_output = None\n",
    "        self.target = None\n",
    "        self.batch_size = None\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        # Handle both single samples and batches\n",
    "        if logits.ndim == 1:\n",
    "            # Single sample case - reshape to batch of size 1\n",
    "            logits = logits.reshape(1, -1)\n",
    "            target = target.reshape(1, -1)\n",
    "\n",
    "        # Apply softmax per sample (along axis=1)\n",
    "        # Subtract max for numerical stability, then exponentiate\n",
    "        exp_logits = cp.exp(logits - cp.max(logits, axis=1, keepdims=True))\n",
    "        # Divide by sum of exponentiated logits per sample (along axis=1)\n",
    "        self.softmax_output = exp_logits / cp.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "        self.target = target\n",
    "        self.batch_size = logits.shape[0]  # Store batch size\n",
    "\n",
    "        # Compute cross entropy loss per sample, then average over the batch\n",
    "        # Use a small epsilon for numerical stability with log(0)\n",
    "        log_softmax = cp.log(self.softmax_output + 1e-15)\n",
    "        # Only consider the log-probabilities of the true classes\n",
    "        loss_per_sample = -cp.sum(\n",
    "            target * log_softmax, axis=1\n",
    "        )  # Sum over classes for each sample\n",
    "\n",
    "        # Return the average loss over the batch\n",
    "        return cp.mean(loss_per_sample)\n",
    "\n",
    "    def backward(self):\n",
    "        grad = (self.softmax_output - self.target) / self.batch_size\n",
    "        return grad\n",
    "\n",
    "    def __call__(self, logits, target):\n",
    "        return self.forward(logits, target)\n",
    "\n",
    "\n",
    "class LeakyReLU:\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return cp.where(x > 0, x, self.alpha * x)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return cp.where(self.input > 0, grad, self.alpha * grad)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=0.01,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        eps=1e-8,\n",
    "    ):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.t = 0  # Global time step, increments once per batch\n",
    "\n",
    "        # Initialize moment estimates based on layer type\n",
    "        self.m = []\n",
    "        self.v = []\n",
    "        for layer in self.params:\n",
    "            if hasattr(layer, \"dendrite_W\"):  # DendriticLayer\n",
    "                self.m.append(\n",
    "                    [\n",
    "                        cp.zeros_like(layer.dendrite_W),\n",
    "                        cp.zeros_like(layer.dendrite_b),\n",
    "                        cp.zeros_like(layer.soma_W),\n",
    "                        cp.zeros_like(layer.soma_b),\n",
    "                    ]\n",
    "                )\n",
    "                self.v.append(\n",
    "                    [\n",
    "                        cp.zeros_like(layer.dendrite_W),\n",
    "                        cp.zeros_like(layer.dendrite_b),\n",
    "                        cp.zeros_like(layer.soma_W),\n",
    "                        cp.zeros_like(layer.soma_b),\n",
    "                    ]\n",
    "                )\n",
    "            else:  # LinearLayer\n",
    "                self.m.append([cp.zeros_like(layer.W), cp.zeros_like(layer.b)])\n",
    "                self.v.append([cp.zeros_like(layer.W), cp.zeros_like(layer.b)])\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for layer in self.params:\n",
    "            if hasattr(layer, \"dendrite_W\"):  # DendriticLayer\n",
    "                layer.dendrite_dW = 0.0\n",
    "                layer.dendrite_db = 0.0\n",
    "                layer.soma_dW = 0.0\n",
    "                layer.soma_db = 0.0\n",
    "            else:  # LinearLayer\n",
    "                layer.dW = 0.0\n",
    "                layer.db = 0.0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1  # Increment global time step\n",
    "        for i, layer in enumerate(self.params):\n",
    "            if hasattr(layer, \"dendrite_W\"):  # DendriticLayer\n",
    "                grads = [\n",
    "                    layer.dendrite_dW,\n",
    "                    layer.dendrite_db,\n",
    "                    layer.soma_dW,\n",
    "                    layer.soma_db,\n",
    "                ]\n",
    "                params = [\n",
    "                    layer.dendrite_W,\n",
    "                    layer.dendrite_b,\n",
    "                    layer.soma_W,\n",
    "                    layer.soma_b,\n",
    "                ]\n",
    "            else:  # LinearLayer\n",
    "                grads = [layer.dW, layer.db]\n",
    "                params = [layer.W, layer.b]\n",
    "\n",
    "            # Update moment estimates and parameters for each parameter\n",
    "            for j, (grad, param) in enumerate(zip(grads, params)):\n",
    "                # Update first moment estimate\n",
    "                self.m[i][j] = self.beta1 * self.m[i][j] + (1 - self.beta1) * grad\n",
    "                # Update second moment estimate\n",
    "                self.v[i][j] = self.beta2 * self.v[i][j] + (1 - self.beta2) * (grad**2)\n",
    "\n",
    "                # Bias correction\n",
    "                m_hat = self.m[i][j] / (1 - self.beta1**self.t)\n",
    "                v_hat = self.v[i][j] / (1 - self.beta2**self.t)\n",
    "\n",
    "                # Update parameters\n",
    "                param -= self.lr * m_hat / (cp.sqrt(v_hat) + self.eps)\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.step()\n",
    "\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def params(self):\n",
    "        \"\"\"Return a list of layers that have a Weight vectors\"\"\"\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, \"W\") or hasattr(layer, \"soma_W\"):\n",
    "                params.append(layer)\n",
    "        return params\n",
    "\n",
    "    def num_params(self):\n",
    "        num_params = 0\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, \"num_params\"):\n",
    "                num_params += layer.num_params()\n",
    "        return num_params\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearLayer:\n",
    "    \"\"\"A fully connected, feed forward layer\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.W = cp.random.randn(out_dim, in_dim) * cp.sqrt(\n",
    "            2.0 / (in_dim)\n",
    "        )  # He init, for ReLU\n",
    "        self.b = cp.zeros(out_dim)\n",
    "        self.dW = 0.0\n",
    "        self.db = 0.0\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.W.T + self.b\n",
    "\n",
    "    def backward(self, grad):\n",
    "        self.dW = grad.T @ self.x\n",
    "        self.db = grad.sum(axis=0)\n",
    "        grad = grad @ self.W\n",
    "        return grad\n",
    "\n",
    "    def num_params(self):\n",
    "        return self.W.size + self.b.size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "\n",
    "class DendriticLayer:\n",
    "    \"\"\"A sparse dendritic layer, consiting of dendrites and somas\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        n_neurons,\n",
    "        n_dendrite_inputs,\n",
    "        n_dendrites,\n",
    "        synaptic_resampling=True,\n",
    "        percentage_resample=0.25,\n",
    "        steps_to_resample=128,\n",
    "    ):\n",
    "        self.in_dim = in_dim\n",
    "        self.n_dendrite_inputs = n_dendrite_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        self.n_dendrites = n_dendrites\n",
    "        self.n_soma_connections = n_dendrites * n_neurons\n",
    "        self.n_synaptic_connections = n_dendrite_inputs * n_dendrites * n_neurons\n",
    "\n",
    "        # dynamicly resample\n",
    "        self.synaptic_resampling = synaptic_resampling\n",
    "        self.percentage_resample = percentage_resample\n",
    "        self.steps_to_resample = steps_to_resample\n",
    "        # to keep track of resampling\n",
    "        self.num_mask_updates = 1\n",
    "        self.update_steps = 0\n",
    "\n",
    "        self.dendrite_W = cp.random.randn(self.n_soma_connections, in_dim) * cp.sqrt(\n",
    "            2.0 / (in_dim)\n",
    "        )  # He init, for ReLU\n",
    "        self.dendrite_b = cp.zeros((self.n_soma_connections))\n",
    "        self.dendrite_dW = 0.0\n",
    "        self.dendrite_db = 0.0\n",
    "        self.dendrite_activation = LeakyReLU()\n",
    "\n",
    "        self.soma_W = cp.random.randn(n_neurons, self.n_soma_connections) * cp.sqrt(\n",
    "            2.0 / (self.n_soma_connections)\n",
    "        )  # He init, for ReLU\n",
    "        self.soma_b = cp.zeros(n_neurons)\n",
    "        self.soma_dW = 0.0\n",
    "        self.soma_db = 0.0\n",
    "        self.soma_activation = LeakyReLU()\n",
    "\n",
    "        # inputs to save for backprop\n",
    "        self.dendrite_x = None\n",
    "        self.soma_x = None\n",
    "\n",
    "        # sample soma mask:\n",
    "        # [[1, 1, 0, 0]\n",
    "        #  [0, 0, 1, 1]]\n",
    "        # number of 1 per row is n_dendrites, rest 0. every column only has 1 entry\n",
    "        # number of rows equals n_neurons, number of columns eqais n_soma_connections\n",
    "        # it is a step pattern, so the first n_dendrites entries of the first row are one.\n",
    "        self.soma_mask = cp.zeros((n_neurons, self.n_soma_connections))\n",
    "        for i in range(n_neurons):\n",
    "            start_idx = i * n_dendrites\n",
    "            end_idx = start_idx + n_dendrites\n",
    "            self.soma_mask[i, start_idx:end_idx] = 1\n",
    "\n",
    "        # mask out unneeded weights, thus making weights sparse\n",
    "        self.soma_W = self.soma_W * self.soma_mask\n",
    "\n",
    "        # sample dendrite mask\n",
    "        # for each dendrite sample n_dendrite_inputs from the input array\n",
    "        self.dendrite_mask = cp.zeros((self.n_soma_connections, in_dim))\n",
    "        for i in range(self.n_soma_connections):\n",
    "            # sample without replacement from possible input for a given dendrite from the whole input\n",
    "            input_idx = cp.random.choice(\n",
    "                cp.arange(in_dim), size=n_dendrite_inputs, replace=False\n",
    "            )\n",
    "            self.dendrite_mask[i, input_idx] = 1\n",
    "\n",
    "        # mask out unneeded weights, thus making weights sparse\n",
    "        self.dendrite_W = self.dendrite_W * self.dendrite_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # dendrites forward pass\n",
    "        self.dendrite_x = x\n",
    "        x = x @ self.dendrite_W.T + self.dendrite_b\n",
    "        x = self.dendrite_activation(x)\n",
    "\n",
    "        # soma forward pass\n",
    "        self.soma_x = x\n",
    "        x = x @ self.soma_W.T + self.soma_b\n",
    "        x = self.soma_activation(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        grad = self.soma_activation.backward(grad)\n",
    "\n",
    "        # soma back pass, multiply with mask to keep only valid gradients\n",
    "        self.soma_dW = grad.T @ self.soma_x * self.soma_mask\n",
    "        self.soma_db = grad.sum(axis=0)\n",
    "        soma_grad = grad @ self.soma_W\n",
    "\n",
    "        soma_grad = self.dendrite_activation.backward(soma_grad)\n",
    "\n",
    "        # dendrite back pass\n",
    "        self.dendrite_dW = soma_grad.T @ self.dendrite_x * self.dendrite_mask\n",
    "        self.dendrite_db = soma_grad.sum(axis=0)\n",
    "        dendrite_grad = soma_grad @ self.dendrite_W\n",
    "\n",
    "        self.update_steps += 1\n",
    "\n",
    "        # resample dendrites every steps_to_resample steps\n",
    "        if self.synaptic_resampling and self.update_steps >= self.steps_to_resample:\n",
    "            # reset step counter\n",
    "            self.update_steps = 0\n",
    "            self.resample_dendrites()\n",
    "\n",
    "        return dendrite_grad\n",
    "\n",
    "    def resample_dendrites(self):\n",
    "        \"\"\"\n",
    "        Implements synaptic resampling by replacing weak dendritic connections with new random ones.\n",
    "\n",
    "        This method mimics synaptic plasticity in biological neurons, where weak or unused\n",
    "        synaptic connections are pruned and replaced with new connections to explore different\n",
    "        input patterns. The resampling helps prevent overfitting and maintains exploration\n",
    "        capabilities during training.\n",
    "\n",
    "        Algorithm Overview:\n",
    "        1. **Connection Removal**: Identifies the weakest connections (lowest weight magnitude)\n",
    "           for removal based on `self.percentage_resample`\n",
    "        2. **One-shot Resampling**: Randomly samples new input connections for each removed connection\n",
    "        3. **Conflict Detection**: Checks for conflicts with existing connections and duplicate\n",
    "           assignments within the same dendrite\n",
    "        4. **Successful Swaps**: Applies only valid swaps that don't create conflicts\n",
    "        5. **Verification**: Ensures the dendritic structure integrity is maintained\n",
    "\n",
    "        The method operates efficiently by:\n",
    "        - Using vectorized operations for all dendrites simultaneously\n",
    "        - Implementing one-shot resampling rather than iterative attempts\n",
    "        - Only applying successful swaps to avoid invalid states\n",
    "        - Maintaining sparsity through the dendrite_mask\n",
    "\n",
    "        Side Effects:\n",
    "            - Updates self.dendrite_mask to reflect new connection patterns\n",
    "            - Reinitializes weights for new connections using He initialization\n",
    "            - Zeros out weights and gradients for removed connections\n",
    "            - Increments self.num_mask_updates counter\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If resampling violates dendritic structure constraints\n",
    "                - Each dendrite must maintain exactly n_dendrite_inputs connections\n",
    "                - Total active connections must equal n_synaptic_connections\n",
    "\n",
    "        Returns:\n",
    "            None: Method modifies the layer's state in-place\n",
    "\n",
    "        Note:\n",
    "            - Called automatically during backward pass if synaptic_resampling=True\n",
    "            - Early returns if percentage_resample results in 0 connections to remove\n",
    "            - Biologically inspired by synaptic pruning and neuroplasticity\n",
    "        \"\"\"\n",
    "        # --- Part 1: Connection Removal ---\n",
    "        # calculate number of connections to remove per dendrite\n",
    "        n_to_remove_per_dendrite = int(\n",
    "            self.n_dendrite_inputs * self.percentage_resample\n",
    "        )\n",
    "        if n_to_remove_per_dendrite == 0:\n",
    "            return\n",
    "\n",
    "        num_dendrites = self.dendrite_mask.shape[0]\n",
    "\n",
    "        # The metric is the magnitude of the weights, removing the smallest.\n",
    "        metric = cp.abs(self.dendrite_W)\n",
    "        # Set inactive connections to infinity so they are not picked.\n",
    "        metric[self.dendrite_mask == 0] = cp.inf\n",
    "        # sort by magnitude\n",
    "        sorted_indices = cp.argsort(metric, axis=1)\n",
    "        # remove the smallest n_to_remove_per_dendrite connections per dendrite\n",
    "        cols_to_remove = sorted_indices[:, :n_to_remove_per_dendrite]\n",
    "\n",
    "        # Create corresponding row indices and flatten for the swap logic\n",
    "        # dummy array of dendrite indices shape (num_dendrites, 1)\n",
    "        rows_to_remove = cp.arange(num_dendrites)[:, cp.newaxis]\n",
    "        # dummy array of shape (num_dendrites , n_to_remove_per_dendrite), flattened\n",
    "        removed_dendrite_indices = rows_to_remove.repeat(\n",
    "            n_to_remove_per_dendrite, axis=1\n",
    "        ).flatten()\n",
    "        removed_input_indices = cols_to_remove.flatten()\n",
    "\n",
    "        # removed_input_indices, contins the indices of the inputs to remove/resample\n",
    "        # for each dendrite in a flattened array\n",
    "\n",
    "        # --- Part 2: One-shot Resampling Attempt ---\n",
    "        n_connections_to_resample = removed_dendrite_indices.size\n",
    "\n",
    "        # sample n_connections_to_resample new inputs between 0 and in_dim\n",
    "        newly_selected_input_indices = cp.random.randint(\n",
    "            0, self.in_dim, size=n_connections_to_resample, dtype=int\n",
    "        )  # shape (n_connections_to_resample, 1)\n",
    "\n",
    "        # --- Part 3: Conflict Detection ---\n",
    "        # check if new inputs are already existing in the same dendrite\n",
    "        conflict_with_existing = (\n",
    "            self.dendrite_mask[removed_dendrite_indices, newly_selected_input_indices]\n",
    "            == 1\n",
    "        )\n",
    "\n",
    "        # create flat indices of proposed new connections\n",
    "        # by multiplying dendrite index with in_dim and adding the input index,\n",
    "        # we get a unique index for each proposed new connection, in  the same dendrite\n",
    "        proposed_flat_indices = (\n",
    "            removed_dendrite_indices * self.in_dim + newly_selected_input_indices\n",
    "        )  # shape (n_connections_to_resample, 1)\n",
    "\n",
    "        # count number of occurences of each index\n",
    "        counts = cp.bincount(\n",
    "            proposed_flat_indices.astype(int),\n",
    "            minlength=num_dendrites * self.in_dim,\n",
    "        )  # shape (num_dendrites * in_dim, 1)\n",
    "\n",
    "        # check if index is duplicated, in the same dendrite\n",
    "        is_duplicate_flat = counts[proposed_flat_indices.astype(int)] > 1\n",
    "\n",
    "        # flag as problematic if either conflict or duplicate\n",
    "        is_problematic = conflict_with_existing | is_duplicate_flat\n",
    "        # flag as successful if not problematic\n",
    "        is_successful = ~is_problematic\n",
    "\n",
    "        # --- Part 4: Apply Successful Swaps ---\n",
    "        dendrites_to_swap = removed_dendrite_indices[is_successful]\n",
    "\n",
    "        # if no successful swaps, return\n",
    "        if dendrites_to_swap.size == 0:\n",
    "            return\n",
    "\n",
    "        # get indices of old and new inputs to remove and add\n",
    "        old_inputs_to_remove = removed_input_indices[is_successful]\n",
    "        new_inputs_to_add = newly_selected_input_indices[is_successful]\n",
    "\n",
    "        # Update mask: remove old connections and add new ones\n",
    "        self.dendrite_mask[dendrites_to_swap, old_inputs_to_remove] = 0\n",
    "        self.dendrite_mask[dendrites_to_swap, new_inputs_to_add] = 1\n",
    "\n",
    "        # Initialize new weights with He initialization\n",
    "        self.dendrite_W[dendrites_to_swap, new_inputs_to_add] = cp.random.randn(\n",
    "            dendrites_to_swap.shape[0]\n",
    "        ) * cp.sqrt(2.0 / self.in_dim)\n",
    "\n",
    "        # Apply mask to ensure only active connections have non-zero weights\n",
    "        self.dendrite_W = self.dendrite_W * self.dendrite_mask\n",
    "        # Also zero out gradients for removed connections\n",
    "        self.dendrite_dW = self.dendrite_dW * self.dendrite_mask\n",
    "\n",
    "        # print(f\"num of dendrite successful swaps: {dendrites_to_swap.size}\")\n",
    "\n",
    "        self.num_mask_updates += 1\n",
    "\n",
    "        # --- Part 5: Verification ---\n",
    "        n_dendritic_mask_connections = cp.sum(self.dendrite_mask, axis=1)\n",
    "        assert cp.all(n_dendritic_mask_connections == self.n_dendrite_inputs), (\n",
    "            f\"Resampling failed: not all dendrites have {self.n_dendrite_inputs} connections per dendrite.\"\n",
    "        )\n",
    "        assert (\n",
    "            cp.sum(cp.count_nonzero(self.dendrite_W)) == self.n_synaptic_connections\n",
    "        ), (\n",
    "            f\"Resampling failed: not all dendrites have {self.n_synaptic_connections} connections in total.\"\n",
    "        )\n",
    "\n",
    "    def num_params(self):\n",
    "        print(\n",
    "            f\"\\nparameters: dendrite_mask: {cp.sum(self.dendrite_mask)}, dendrite_b: {self.dendrite_b.size}, soma_W: {cp.sum(self.soma_mask)}, soma_b: {self.soma_b.size}\"\n",
    "        )\n",
    "        return int(\n",
    "            cp.sum(self.dendrite_mask)\n",
    "            + self.dendrite_b.size\n",
    "            + cp.sum(self.soma_mask)\n",
    "            + self.soma_b.size\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "\n",
    "def load_mnist_data(\n",
    "    dataset=\"mnist\",\n",
    "    subset_size=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Download and load the MNIST or Fashion-MNIST dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): Dataset to load - either \"mnist\" or \"fashion-mnist\"\n",
    "        normalize (bool): If True, normalize pixel values to [0, 1]\n",
    "        flatten (bool): If True, flatten 28x28 images to 784-dimensional vectors\n",
    "        one_hot (bool): If True, convert labels to one-hot encoding\n",
    "        subset_size (int): If specified, return only a subset of the data\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train, y_train, X_test, y_test)\n",
    "            X_train, X_test: Input features\n",
    "            y_train, y_test: Target labels\n",
    "    \"\"\"\n",
    "    # Map dataset names to OpenML dataset identifiers\n",
    "    dataset_mapping = {\"mnist\": \"mnist_784\", \"fashion-mnist\": \"Fashion-MNIST\"}\n",
    "\n",
    "    if dataset not in dataset_mapping:\n",
    "        raise ValueError(\n",
    "            f\"Dataset must be one of {list(dataset_mapping.keys())}, got '{dataset}'\"\n",
    "        )\n",
    "\n",
    "    dataset_name = dataset_mapping[dataset]\n",
    "    print(f\"Loading {dataset.upper()} dataset...\")\n",
    "\n",
    "    # Download dataset\n",
    "    data = fetch_openml(\n",
    "        dataset_name, version=1, as_frame=False, parser=\"auto\", cache=True\n",
    "    )\n",
    "\n",
    "    X, y = data.data, data.target.astype(int)\n",
    "\n",
    "    # Split into train and test (last 10k samples for test, rest for train)\n",
    "    X_train, X_test = X[:60000], X[60000:]\n",
    "    y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "    # Normalize pixel values and convert to GPU arrays\n",
    "    # Convert to float32 first\n",
    "    X_train = X_train.astype(np.float32) / 255.0\n",
    "    X_test = X_test.astype(np.float32) / 255.0\n",
    "\n",
    "    # Calculate global mean and std from training data\n",
    "    mean_val = X_train.mean()\n",
    "    std_val = X_train.std()\n",
    "\n",
    "    # Standardize to mean=0, std=1\n",
    "    X_train = (X_train - mean_val) / std_val\n",
    "    X_test = (X_test - mean_val) / std_val\n",
    "\n",
    "    # Convert to CuPy arrays\n",
    "    X_train = cp.array(X_train)\n",
    "    X_test = cp.array(X_test)\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    def to_one_hot(labels, n_classes=10):\n",
    "        one_hot_labels = cp.zeros((len(labels), n_classes))\n",
    "        one_hot_labels[cp.arange(len(labels)), labels] = 1\n",
    "        return one_hot_labels\n",
    "\n",
    "    y_train = to_one_hot(cp.array(y_train))\n",
    "    y_test = to_one_hot(cp.array(y_test))\n",
    "\n",
    "    # Use subset if specified\n",
    "    if subset_size is not None:\n",
    "        X_train, y_train = X_train[:subset_size], y_train[:subset_size]\n",
    "        X_test, y_test = (\n",
    "            X_test[: subset_size // 6],\n",
    "            y_test[: subset_size // 6],\n",
    "        )  # Keep proportional test size\n",
    "\n",
    "    print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def create_batches(X, y, batch_size=128, shuffle=True, drop_last=False):\n",
    "    n_samples = len(X)\n",
    "    # shuffle data\n",
    "    if shuffle:\n",
    "        indices = cp.arange(n_samples)\n",
    "        cp.random.shuffle(indices)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        if drop_last and i + batch_size > n_samples:\n",
    "            break\n",
    "        X_batch = X[i : i + batch_size]\n",
    "        y_batch = y[i : i + batch_size]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "\n",
    "def train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimiser,\n",
    "    n_epochs=2,\n",
    "    batch_size=128,\n",
    "):\n",
    "    train_losses = []\n",
    "    accuracy = []\n",
    "    test_losses = []\n",
    "    test_accuracy = []\n",
    "    n_samples = len(X_train)\n",
    "    num_batches_per_epoch = (n_samples + batch_size - 1) // batch_size\n",
    "    total_batches = n_epochs * num_batches_per_epoch\n",
    "\n",
    "    with tqdm(total=total_batches, desc=\"Training \") as pbar:\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = 0.0\n",
    "            correct_pred = 0.0\n",
    "            for batch_idx, (X, target) in enumerate(\n",
    "                create_batches(\n",
    "                    X_train, y_train, batch_size, shuffle=True, drop_last=True\n",
    "                )\n",
    "            ):\n",
    "                # forward pass\n",
    "                pred = model(X)\n",
    "                batch_loss = criterion(pred, target)\n",
    "                train_loss += batch_loss\n",
    "                # if most likely prediction equals target add to correct predictions\n",
    "                batch_correct = cp.sum(\n",
    "                    cp.argmax(pred, axis=1) == cp.argmax(target, axis=1)\n",
    "                )\n",
    "                correct_pred += batch_correct\n",
    "\n",
    "                # backward pass\n",
    "                optimiser.zero_grad()\n",
    "                grad = criterion.backward()\n",
    "                model.backward(grad)\n",
    "                optimiser.step()\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"Epoch\": f\"{epoch + 1}/{n_epochs}\",\n",
    "                        \"Batch\": f\"{batch_idx + 1}/{num_batches_per_epoch}\",\n",
    "                        \"Loss\": f\"{float(batch_loss):.4f}\",\n",
    "                    }\n",
    "                )\n",
    "                pbar.update(1)\n",
    "            # evaluate on test set\n",
    "            epoch_test_loss, epoch_test_accuracy = evaluate(\n",
    "                X_test, y_test, model, criterion\n",
    "            )\n",
    "            normalised_train_loss = train_loss / num_batches_per_epoch\n",
    "            train_losses.append(float(normalised_train_loss))\n",
    "            epoch_accuracy = correct_pred / n_samples\n",
    "            accuracy.append(float(epoch_accuracy))\n",
    "            test_losses.append(float(epoch_test_loss))\n",
    "            test_accuracy.append(float(epoch_test_accuracy))\n",
    "    return train_losses, accuracy, test_losses, test_accuracy\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    model,\n",
    "    criterion,\n",
    "    batch_size=1024,  # higher batch size for testing\n",
    "):\n",
    "    n_samples = len(X_test)\n",
    "    test_loss = 0.0\n",
    "    correct_pred = 0.0\n",
    "    num_batches_per_epoch = (n_samples + batch_size - 1) // batch_size\n",
    "    for X, target in create_batches(X_test, y_test, batch_size, shuffle=False):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        batch_loss = criterion(pred, target)\n",
    "        test_loss += batch_loss\n",
    "        # if most likely prediction equals target add to correct predictions\n",
    "        batch_correct = cp.sum(cp.argmax(pred, axis=1) == cp.argmax(target, axis=1))\n",
    "        correct_pred += batch_correct\n",
    "    normalised_test_loss = test_loss / num_batches_per_epoch\n",
    "    accuracy = correct_pred / n_samples\n",
    "    return float(normalised_test_loss), float(accuracy)\n",
    "\n",
    "\n",
    "def plot_dendritic_weights_single_image(\n",
    "    model, input_image, neuron_idx=0, image_shape=(28, 28)\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the aggregated magnitude of all dendritic weights of a single neuron on one image.\n",
    "    Color indicates the sum of magnitudes at each location.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_numpy(arr):\n",
    "        if hasattr(arr, \"get\"):\n",
    "            return arr.get()\n",
    "        return np.asarray(arr)\n",
    "\n",
    "    # Find the first DendriticLayer\n",
    "    dendritic_layer = None\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, \"dendrite_W\"):\n",
    "            dendritic_layer = layer\n",
    "            break\n",
    "\n",
    "    if dendritic_layer is None:\n",
    "        print(\"No DendriticLayer found in the model.\")\n",
    "        return\n",
    "\n",
    "    if not (0 <= neuron_idx < dendritic_layer.n_neurons):\n",
    "        print(\n",
    "            f\"Invalid neuron_idx. Must be between 0 and {dendritic_layer.n_neurons - 1}.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Get the weights and mask for the specified neuron's dendrites\n",
    "    start_idx = neuron_idx * dendritic_layer.n_dendrites\n",
    "    end_idx = start_idx + dendritic_layer.n_dendrites\n",
    "\n",
    "    dendrite_weights = to_numpy(dendritic_layer.dendrite_W[start_idx:end_idx])\n",
    "    dendrite_mask = to_numpy(dendritic_layer.dendrite_mask[start_idx:end_idx])\n",
    "\n",
    "    masked_weights = dendrite_weights * dendrite_mask\n",
    "    input_image_np = to_numpy(input_image)\n",
    "\n",
    "    # Calculate and sum magnitudes\n",
    "    magnitudes = np.abs(masked_weights)\n",
    "    summed_magnitudes = np.sum(magnitudes, axis=0)\n",
    "\n",
    "    # Reshape for plotting\n",
    "    summed_magnitudes_2d = summed_magnitudes.reshape(image_shape)\n",
    "\n",
    "    # Plot background image\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(input_image_np.reshape(image_shape), cmap=\"gray\", interpolation=\"nearest\")\n",
    "\n",
    "    # Mask zeros for the overlay\n",
    "    heatmap_masked = np.ma.masked_where(summed_magnitudes_2d == 0, summed_magnitudes_2d)\n",
    "\n",
    "    # Plot heatmap of magnitudes\n",
    "    im = ax.imshow(heatmap_masked, cmap=\"viridis\", alpha=0.6, interpolation=\"nearest\")\n",
    "\n",
    "    # Add colorbar\n",
    "    fig.colorbar(im, ax=ax, label=\"Sum of Weight Magnitudes\")\n",
    "\n",
    "    ax.set_title(f\"Aggregated Dendritic Weight Magnitudes for Neuron {neuron_idx}\")\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_dendritic_weights_full_model(model, image_shape=(28, 28)):\n",
    "    \"\"\"\n",
    "    Plots the aggregated magnitude of all dendritic weights across all neurons in the model.\n",
    "    Shows the combined weight pattern without any background image.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_numpy(arr):\n",
    "        if hasattr(arr, \"get\"):\n",
    "            return arr.get()\n",
    "        return np.asarray(arr)\n",
    "\n",
    "    # Find the DendriticLayer\n",
    "    dendritic_layer = None\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, \"dendrite_W\"):\n",
    "            dendritic_layer = layer\n",
    "            break\n",
    "\n",
    "    if dendritic_layer is None:\n",
    "        print(\"No DendriticLayer found in the model.\")\n",
    "        return\n",
    "\n",
    "    n_neurons = dendritic_layer.n_neurons\n",
    "    n_dendrites = dendritic_layer.n_dendrites\n",
    "\n",
    "    print(f\"Visualizing {n_neurons} neurons, {n_dendrites} dendrites each\")\n",
    "\n",
    "    # Get all dendritic weights and masks\n",
    "    dendrite_weights = to_numpy(dendritic_layer.dendrite_W)\n",
    "    dendrite_mask = to_numpy(dendritic_layer.dendrite_mask)\n",
    "\n",
    "    # Apply mask to get only active weights\n",
    "    masked_weights = dendrite_weights * dendrite_mask\n",
    "\n",
    "    # Calculate magnitudes and sum across all dendrites and neurons\n",
    "    magnitudes = np.abs(masked_weights)\n",
    "    total_magnitudes = np.sum(magnitudes, axis=0)\n",
    "\n",
    "    # Reshape for plotting\n",
    "    total_magnitudes_2d = total_magnitudes.reshape(image_shape)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Plot heatmap of all dendritic weights\n",
    "    im = ax.imshow(total_magnitudes_2d, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "\n",
    "    # Add colorbar\n",
    "    fig.colorbar(im, ax=ax, label=\"Sum of All Dendritic Weight Magnitudes\")\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"All Dendritic Weights Aggregated\\nTotal Neurons: {n_neurons}, Dendrites per Neuron: {n_dendrites}\"\n",
    "    )\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Also create a subplot showing individual neuron contributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    fig.suptitle(\"Dendritic Weight Analysis - Full Model\", fontsize=16)\n",
    "\n",
    "    # Plot 1: All weights aggregated\n",
    "    im1 = axes[0, 0].imshow(\n",
    "        total_magnitudes_2d, cmap=\"viridis\", interpolation=\"nearest\"\n",
    "    )\n",
    "    axes[0, 0].set_title(\"All Dendritic Weights\")\n",
    "    axes[0, 0].axis(\"off\")\n",
    "    fig.colorbar(im1, ax=axes[0, 0], shrink=0.7)\n",
    "    # Plot 2: Weight distribution histogram\n",
    "    axes[0, 1].hist(\n",
    "        magnitudes[magnitudes > 0],\n",
    "        bins=50,\n",
    "        alpha=0.7,\n",
    "        color=\"blue\",\n",
    "        edgecolor=\"black\",\n",
    "        range=(0, 1.2),\n",
    "    )\n",
    "    axes[0, 1].set_title(f\"Weight Magnitude Distribution (Total Weights: {magnitudes.size})\")\n",
    "    axes[0, 1].set_xlabel(\"Weight Magnitude\")\n",
    "    axes[0, 1].set_ylabel(\"Frequency\")\n",
    "    axes[0, 1].set_ylim(0, 1200)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Spatial activity map (sum of absolute weights at each location)\n",
    "    spatial_activity = np.sum(np.abs(masked_weights), axis=0).reshape(image_shape)\n",
    "    im3 = axes[1, 0].imshow(spatial_activity, cmap=\"plasma\", interpolation=\"nearest\")\n",
    "    axes[1, 0].set_title(\"Spatial Activity Map\")\n",
    "    axes[1, 0].axis(\"off\")\n",
    "    fig.colorbar(im3, ax=axes[1, 0], shrink=0.7)\n",
    "\n",
    "    # Plot 4: Active connections map (count of non-zero weights at each location)\n",
    "    active_connections = np.sum(masked_weights != 0, axis=0).reshape(image_shape)\n",
    "    im4 = axes[1, 1].imshow(active_connections, cmap=\"hot\", interpolation=\"nearest\")\n",
    "    axes[1, 1].set_title(\"Active Connections Count\")\n",
    "    axes[1, 1].axis(\"off\")\n",
    "    fig.colorbar(im4, ax=axes[1, 1], shrink=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print some statistics\n",
    "    print(\"\\n=== Full Model Dendritic Statistics ===\")\n",
    "    print(f\"Total parameters: {dendrite_weights.size}\")\n",
    "    print(f\"Active parameters: {np.sum(dendrite_mask)}\")\n",
    "    print(f\"Sparsity: {1 - np.sum(dendrite_mask) / dendrite_mask.size:.4f}\")\n",
    "    print(f\"Mean active weight magnitude: {np.mean(magnitudes[magnitudes > 0]):.6f}\")\n",
    "    print(f\"Max weight magnitude: {np.max(magnitudes):.6f}\")\n",
    "    print(f\"Min non-zero weight magnitude: {np.min(magnitudes[magnitudes > 0]):.6f}\")\n",
    "\n",
    "\n",
    "# for repoducability\n",
    "cp.random.seed(1287305233)\n",
    "\n",
    "# data config\n",
    "dataset = \"fashion-mnist\"  # \"mnist\", \"fashion-mnist\"\n",
    "\n",
    "# config\n",
    "n_epochs = 20  # 15 MNIST, 20 Fashion-MNIST\n",
    "lr = 0.001  # 0.002\n",
    "v_lr = 0.001  # 0.002\n",
    "b_lr = 0.001  # 0.002\n",
    "batch_size = 128\n",
    "\n",
    "in_dim = 28 * 28  # Image dimensions (28x28 MNIST)\n",
    "n_classes = 10\n",
    "\n",
    "# dendritic model config\n",
    "n_dendrite_inputs = 16  # 32\n",
    "n_dendrites = 31  # 23\n",
    "n_neurons = 14  # 10\n",
    "\n",
    "# vanilla model config\n",
    "hidden_dim = 10  # 10\n",
    "\n",
    "model_name_1 = \"Synaptic Resampling\"\n",
    "model_name_2 = \"Base Dendritic\"\n",
    "model_name_3 = \"Vanilla ANN\"\n",
    "\n",
    "criterion = CrossEntropy()\n",
    "\n",
    "# new model with synaptic resampling\n",
    "model_1 = Sequential(\n",
    "    [\n",
    "        DendriticLayer(\n",
    "            in_dim,\n",
    "            n_neurons,\n",
    "            n_dendrite_inputs=n_dendrite_inputs,\n",
    "            n_dendrites=n_dendrites,\n",
    "            synaptic_resampling=True,\n",
    "            percentage_resample=0.1,  # 0.1\n",
    "            steps_to_resample=128,  # 128\n",
    "        ),\n",
    "        LeakyReLU(),\n",
    "        LinearLayer(n_neurons, n_classes),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# baseline dendritic model without synaptic resampling\n",
    "model_2 = Sequential(\n",
    "    [\n",
    "        DendriticLayer(\n",
    "            in_dim,\n",
    "            n_neurons,\n",
    "            n_dendrite_inputs=n_dendrite_inputs,\n",
    "            n_dendrites=n_dendrites,\n",
    "            synaptic_resampling=False,\n",
    "        ),\n",
    "        LeakyReLU(),\n",
    "        LinearLayer(n_neurons, n_classes),\n",
    "    ]\n",
    ")\n",
    "# vanilla ANN model\n",
    "model_3 = Sequential(\n",
    "    [\n",
    "        LinearLayer(in_dim, hidden_dim),\n",
    "        LeakyReLU(),\n",
    "        LinearLayer(hidden_dim, hidden_dim),\n",
    "        LeakyReLU(),\n",
    "        LinearLayer(hidden_dim, n_classes),\n",
    "    ]\n",
    ")\n",
    "optimiser_1 = Adam(model_1.params(), lr=lr)\n",
    "optimiser_2 = Adam(model_2.params(), lr=b_lr)\n",
    "optimiser_3 = Adam(model_3.params(), lr=v_lr)\n",
    "\n",
    "print(f\"number of model_1 params: {model_1.num_params()}\")\n",
    "print(f\"number of model_2 params: {model_2.num_params()}\")\n",
    "print(f\"number of model_3 params: {model_3.num_params()}\")\n",
    "\n",
    "# load data\n",
    "X_train, y_train, X_test, y_test = load_mnist_data(dataset=dataset)\n",
    "\n",
    "print(f\"Training {model_name_1} model...\")\n",
    "train_losses_1, train_accuracy_1, test_losses_1, test_accuracy_1 = train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    model_1,\n",
    "    criterion,\n",
    "    optimiser_1,\n",
    "    n_epochs,\n",
    "    batch_size,\n",
    ")\n",
    "print(f\"train accuracy {model_name_1} model {round(train_accuracy_1[-1] * 100, 1)}%\")\n",
    "print(f\"test accuracy {model_name_1} model {round(test_accuracy_1[-1] * 100, 1)}%\")\n",
    "\n",
    "print(f\"Training {model_name_2} model...\")\n",
    "train_losses_2, train_accuracy_2, test_losses_2, test_accuracy_2 = train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    model_2,\n",
    "    criterion,\n",
    "    optimiser_2,\n",
    "    n_epochs,\n",
    "    batch_size,\n",
    ")\n",
    "\n",
    "print(f\"Training {model_name_3} model...\")\n",
    "train_losses_3, train_accuracy_3, test_losses_3, test_accuracy_3 = train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    model_3,\n",
    "    criterion,\n",
    "    optimiser_3,\n",
    "    n_epochs,\n",
    "    batch_size,\n",
    ")\n",
    "\n",
    "# plot accuracy of vanilla model vs dendritic model\n",
    "plt.plot(train_accuracy_1, label=f\"{model_name_1} Train\", color=\"green\", linestyle=\"--\")\n",
    "plt.plot(train_accuracy_2, label=f\"{model_name_2} Train\", color=\"blue\", linestyle=\"--\")\n",
    "plt.plot(train_accuracy_3, label=f\"{model_name_3} Train\", color=\"red\", linestyle=\"--\")\n",
    "plt.plot(test_accuracy_1, label=f\"{model_name_1} Test\", color=\"green\")\n",
    "plt.plot(test_accuracy_2, label=f\"{model_name_2} Test\", color=\"blue\")\n",
    "plt.plot(test_accuracy_3, label=f\"{model_name_3} Test\", color=\"red\")\n",
    "plt.title(\"Accuracy over epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot both models in comparison\n",
    "plt.plot(train_losses_1, label=f\"{model_name_1} Train\", color=\"green\", linestyle=\"--\")\n",
    "plt.plot(train_losses_2, label=f\"{model_name_2} Train\", color=\"blue\", linestyle=\"--\")\n",
    "plt.plot(train_losses_3, label=f\"{model_name_3} Train\", color=\"red\", linestyle=\"--\")\n",
    "plt.plot(test_losses_1, label=f\"{model_name_1} Test\", color=\"green\")\n",
    "plt.plot(test_losses_2, label=f\"{model_name_2} Test\", color=\"blue\")\n",
    "plt.plot(test_losses_3, label=f\"{model_name_3} Test\", color=\"red\")\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"train loss {model_name_1} model {round(train_losses_1[-1], 4)} vs {model_name_2} {round(train_losses_2[-1], 4)} vs {model_name_3} {round(train_losses_3[-1], 4)}\"\n",
    ")\n",
    "print(\n",
    "    f\"test loss {model_name_1} model {round(test_losses_1[-1], 4)} vs {model_name_2} {round(test_losses_2[-1], 4)} vs {model_name_3} {round(test_losses_3[-1], 4)}\"\n",
    ")\n",
    "print(\n",
    "    f\"train accuracy {model_name_1} model {round(train_accuracy_1[-1] * 100, 1)}% vs {model_name_2} {round(train_accuracy_2[-1] * 100, 1)}% vs {model_name_3} {round(train_accuracy_3[-1] * 100, 1)}%\"\n",
    ")\n",
    "print(\n",
    "    f\"test accuracy {model_name_1} model {round(test_accuracy_1[-1] * 100, 1)}% vs {model_name_2} {round(test_accuracy_2[-1] * 100, 1)}% vs {model_name_3} {round(test_accuracy_3[-1] * 100, 1)}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28706df8-ca04-4235-8ea2-ceeab4b6a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dendritic_weights_single_image(model_1, X_train[0], neuron_idx=4)\n",
    "plot_dendritic_weights_full_model(model_1)\n",
    "plot_dendritic_weights_full_model(model_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
